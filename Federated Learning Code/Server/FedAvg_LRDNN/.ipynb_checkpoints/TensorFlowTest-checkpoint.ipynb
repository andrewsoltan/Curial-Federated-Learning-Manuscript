{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d813399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, recall_score, precision_score, accuracy_score,roc_auc_score, confusion_matrix, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from scipy.special import ndtri\n",
    "from math import sqrt\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "\n",
    "import utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9925f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing and pre-processing training dataset\n",
      "n patients:\t\t112394\n",
      "Loading validation cohort\n",
      "n patients:\t\t22857\n",
      "Preparing training cohorts\n"
     ]
    }
   ],
   "source": [
    "#Quickstart code for experimenting\n",
    "\n",
    "##############################################\n",
    "# Global configuration: FL server parameters #\n",
    "##############################################\n",
    "hostName = \"localhost\"\n",
    "port = \"8002\"\n",
    "fastStart = True #Set to False on new deployment\n",
    "\n",
    "##############################################\n",
    "# SITE-SPECIFIC CONFIGURATION #\n",
    "##############################################\n",
    "global siteName\n",
    "siteName = \"OUH\"\n",
    "trainingOnly = False;\n",
    "pathToTrainingData = \"../raw/df_train_optionC_noaug.csv\"\n",
    "pathToValidationData = \"../raw/OUH Wave 2 Attendances with LFTs Untested Removed.csv\"\n",
    "\n",
    "#Import pre_processing for site-specific dataset from raw file\n",
    "sys.path.insert(0, '../preproc')\n",
    "import preprocess_OUH as preprocessor\n",
    "\n",
    "##############################\n",
    "# Analysis constants #\n",
    "##############################\n",
    "featureSet = \"Bloods & Vitals\" #, 'OLO & Vitals', 'Bloods & Blood_Gas & Vitals'];\n",
    "imputationMethod = \"median\"\n",
    "match_number = 10 #Match number for controls to cases during training\n",
    "localpath = ''\n",
    "resultsPath = 'Results/'\n",
    "\n",
    "##############################\n",
    "# Evaluation Metrics #\n",
    "##############################\n",
    "metric_names = ['Recall','Precision','F1-Score','Accuracy','Specificity','PPV','NPV','AUC']\n",
    "metrics = [recall_score,precision_score,accuracy_score,roc_auc_score]#,confusion_matrix]\n",
    "metrics_dict = dict(zip(metric_names,metrics))\n",
    "n_folds = 10 #Number of folds to use during Cross Validation\n",
    "alpha = 0.95\n",
    "targetSensitivity = 0.90\n",
    "\n",
    "##############\n",
    "#Stage 1: Run pre-processing script from raw dataset\n",
    "print ('Importing and pre-processing training dataset')\n",
    "#First pre-process training set\n",
    "preprocessedTrainingDataset = preprocessor.preprocess(pathToTrainingData)\n",
    "\n",
    "##############\n",
    "#Stage 2: Formation of correct training & validation cohorts (where applicable), noramlisation & imputation\n",
    "\n",
    "#Restrict Population to patients admitted to Hospital\n",
    "preprocessedTrainingDataset = preprocessedTrainingDataset[(preprocessedTrainingDataset.Admission == 1.0) | (preprocessedTrainingDataset.ICU == 1.0)]\n",
    "\n",
    "#Perform split in to train and validation cohorts where applicable\n",
    "if trainingOnly:\n",
    "    trainingDataset = preprocessedTrainingDataset\n",
    "elif not trainingOnly:\n",
    "    #Perform Test/Validate split if not in same file\n",
    "    #Or alternatively, load in separate file and pre-process\n",
    "    trainingDataset = preprocessedTrainingDataset\n",
    "    print ('Loading validation cohort')\n",
    "    validationDataset = preprocessor.preprocess(pathToValidationData)\n",
    "    #Restrict validation to patients who had a Covid-19 test result\n",
    "    validationDataset = validationDataset[(validationDataset['Covid-19 Positive'] == 1.0) | (validationDataset['Covid-19 Positive'] == 0.0)]\n",
    "\n",
    "print ('Preparing training cohorts')\n",
    "#Define pandemic and pre-pandemic cohorts\n",
    "prepandemic = trainingDataset[trainingDataset.ArrivalDateTime < '2019-12-01']\n",
    "pandemic = trainingDataset[trainingDataset.ArrivalDateTime > '2019-12-01']\n",
    "\n",
    "#Generate the full training set; with cases and matched controls\n",
    "\"\"\" Obtain Case Cohort \"\"\"\n",
    "def load_case_data(data):\n",
    "    condition = data['Covid-19 Positive']==True\n",
    "    case_indices = data.index[condition]\n",
    "    case_data = data.loc[case_indices,:]\n",
    "    return case_data,case_indices\n",
    "\n",
    "\"\"\" Obtain Whole Control Cohort \"\"\"\n",
    "def load_control_data(data):\n",
    "\n",
    "    condition = data['ArrivalDateTime'] < '2019-12-01'\n",
    "    control_indicies = data.index[condition]\n",
    "    control_data = data.loc[control_indicies,:]\n",
    "    return control_data,control_indicies\n",
    "\n",
    "\"\"\" Obtain Matched Control Cohort, Matched for Age (+/- 4 years), Gender and Ethnicity \"\"\"\n",
    "def load_matched_control_cohort(match_number,control_data,data,case_indices):\n",
    "    matched_cohort_indices = []\n",
    "    for index in case_indices:\n",
    "        patient_data = data.loc[index,:]\n",
    "        patient_age = patient_data['Age']\n",
    "        gender = patient_data['Gender']\n",
    "        ethnicity = patient_data['Ethnicity']\n",
    "\n",
    "        age_condition1 = control_data['Age'] < patient_age + 4\n",
    "        age_condition2 = control_data['Age'] > patient_age - 4\n",
    "        gender_condition = control_data['Gender'] == gender\n",
    "        ethnicity_condition = control_data['Ethnicity'] == ethnicity\n",
    "\n",
    "        matched_indices = control_data.index[age_condition1 & age_condition2 & gender_condition & ethnicity_condition]\n",
    "        matched_indices = matched_indices.tolist()\n",
    "        random.seed(0)\n",
    "        matched_indices = random.sample(matched_indices,len(matched_indices))\n",
    "\n",
    "        valid_indices = [index for index in matched_indices if index not in matched_cohort_indices][:match_number]\n",
    "        matched_cohort_indices.extend(valid_indices)\n",
    "        control_cohort = control_data.loc[matched_cohort_indices,:] #index name not location\n",
    "\n",
    "    return control_cohort\n",
    "\n",
    "\"\"\" Combine cases & control cohorts \"\"\"\n",
    "def load_combined_cohort(cases,controls):\n",
    "    df = pd.concat((cases, controls),0)\n",
    "    return df\n",
    "\n",
    "#Quickstart File name\n",
    "name = 'quickstart_' + siteName + ' Training Set.pkl'\n",
    "\n",
    "if not fastStart:\n",
    "    #Generate cohorts of cases, controls, and matched controls\n",
    "    cases, case_indices = load_case_data (trainingDataset)\n",
    "    controls, control_indicies = load_control_data (trainingDataset)\n",
    "    matched_controls = load_matched_control_cohort(match_number,controls,trainingDataset,case_indices)\n",
    "\n",
    "    #Combine matched controls with cases to give a full training set\n",
    "    caseControlTrainingSet = load_combined_cohort(cases,matched_controls)\n",
    "\n",
    "    with open(os.path.join(localpath,name),'wb') as f:\n",
    "        pickle.dump(caseControlTrainingSet,f)\n",
    "elif fastStart:\n",
    "    caseControlTrainingSet = pd.read_pickle(name)\n",
    "\n",
    "\n",
    "#The training dataset is now ready - cases are now in 'cases', and controls are now in 'matched_controls' - stored in trainingDataSet\n",
    "\"\"\" Return the relevant feature list  \"\"\"\n",
    "def featureList(featureSet):\n",
    "    if featureSet == \"OLO & Vitals\":\n",
    "        featureList = pd.Series(['Blood_Test BASOPHILS', 'Blood_Test EOSINOPHILS', 'Blood_Test HAEMATOCRIT', 'Blood_Test HAEMOGLOBIN', 'Blood_Test LYMPHOCYTES', 'Blood_Test MEAN CELL VOL.', 'Blood_Test MONOCYTES', 'Blood_Test NEUTROPHILS', 'Blood_Test PLATELETS', 'Blood_Test WHITE CELLS', 'Vital_Sign Respiratory Rate', 'Vital_Sign Heart Rate', 'Vital_Sign Systolic Blood Pressure', 'Vital_Sign Temperature Tympanic', 'Vital_Sign Oxygen Saturation', 'Vital_Sign Delivery device used', 'Vital_Sign Diastolic Blood Pressure'])\n",
    "    elif featureSet == \"Bloods & Vitals\":\n",
    "        featureList = pd.Series(['Blood_Test ALBUMIN', 'Blood_Test ALK.PHOSPHATASE', 'Blood_Test ALT', 'Blood_Test BASOPHILS', 'Blood_Test BILIRUBIN', 'Blood_Test CREATININE', 'Blood_Test CRP', 'Blood_Test EOSINOPHILS', 'Blood_Test HAEMATOCRIT', 'Blood_Test HAEMOGLOBIN', 'Blood_Test LYMPHOCYTES', 'Blood_Test MEAN CELL VOL.', 'Blood_Test MONOCYTES', 'Blood_Test NEUTROPHILS', 'Blood_Test PLATELETS', 'Blood_Test POTASSIUM', 'Blood_Test SODIUM', 'Blood_Test UREA', 'Blood_Test WHITE CELLS', 'Blood_Test eGFR', 'Vital_Sign Respiratory Rate', 'Vital_Sign Heart Rate', 'Vital_Sign Systolic Blood Pressure', 'Vital_Sign Temperature Tympanic', 'Vital_Sign Oxygen Saturation', 'Vital_Sign Delivery device used', 'Vital_Sign Diastolic Blood Pressure'])\n",
    "    elif featureSet == \"Bloods & Blood_Gas & Vitals\":\n",
    "        featureList = pd.Series(['Blood_Test ALBUMIN', 'Blood_Test ALK.PHOSPHATASE', 'Blood_Test ALT', 'Blood_Test APTT', 'Blood_Test BASOPHILS', 'Blood_Test BILIRUBIN', 'Blood_Test CREATININE', 'Blood_Test CRP', 'Blood_Test EOSINOPHILS', 'Blood_Test HAEMATOCRIT', 'Blood_Test HAEMOGLOBIN', 'Blood_Test LYMPHOCYTES', 'Blood_Test MEAN CELL VOL.', 'Blood_Test MONOCYTES', 'Blood_Test NEUTROPHILS', 'Blood_Test PLATELETS', 'Blood_Test POTASSIUM', 'Blood_Test Prothromb. Time', 'Blood_Test SODIUM', 'Blood_Test UREA', 'Blood_Test WHITE CELLS', 'Blood_Test eGFR', 'Blood_Gas BE Std (BG)', 'Blood_Gas Bicarb (BG)', 'Blood_Gas Ca+ + (BG)', 'Blood_Gas cLAC (BG)', 'Blood_Gas Glucose (BG)', 'Blood_Gas Hb (BG)', 'Blood_Gas Hct (BG)', 'Blood_Gas K+ (BG)', 'Blood_Gas Na+ (BG)', 'Blood_Gas O2 Sat (BG)', 'Blood_Gas pCO2 POC', 'Blood_Gas pO2 (BG)', 'Vital_Sign Respiratory Rate', 'Vital_Sign Heart Rate', 'Vital_Sign Systolic Blood Pressure', 'Vital_Sign Temperature Tympanic', 'Vital_Sign Oxygen Saturation', 'Vital_Sign Delivery device used', 'Vital_Sign Diastolic Blood Pressure'])\n",
    "    return featureList\n",
    "\n",
    "\"\"\" Generate X & Y for training  \"\"\"\n",
    "def load_data_splits(df,featureList):\n",
    "        X = df[featureList]\n",
    "        Y = df['Covid-19 Positive']\n",
    "        return X,Y\n",
    "\n",
    "#Get Features, generate X & Y\n",
    "features = featureList(featureSet)\n",
    "X,Y = load_data_splits(caseControlTrainingSet, features)\n",
    "\n",
    "#Now impute missing features, and perform standardization\n",
    "#For the purposes of federated evaluation, the imputer and scaler are stored in imputer and scaler\n",
    "\n",
    "\"\"\" Train imputer on training set \"\"\"\n",
    "def impute_missing_features(imputation_method,X,siteName):\n",
    "    if imputation_method in ['mean','median','MICE']:\n",
    "        if imputation_method == 'mean':\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "        elif imputation_method == 'median':\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "        imputer.fit(X)\n",
    "        X = pd.DataFrame(imputer.transform(X), columns=X.columns)\n",
    "\n",
    "    name = 'imputer_' + siteName + '_'+imputation_method+'.pkl'\n",
    "    with open(os.path.join(localpath,name),'wb') as f:\n",
    "        pickle.dump(imputer,f)\n",
    "    return X, imputer\n",
    "\n",
    "\"\"\" Standardize features \"\"\"\n",
    "def standardize_features(X, siteName):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    name = 'norm_' + siteName + '.pkl'\n",
    "    with open(os.path.join(localpath,name),'wb') as f:\n",
    "        pickle.dump(scaler,f)\n",
    "\n",
    "    X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "    return X, scaler\n",
    "\n",
    "#Perform imputation & scaling\n",
    "X, imputer = impute_missing_features(imputationMethod,X,siteName)\n",
    "X, scaler = standardize_features(X,siteName)\n",
    "\n",
    "#Fill Y missing values with 0, as these are all true negatives\n",
    "Y[np.isnan(Y)] = 0\n",
    "\n",
    "#Now perform an 80/20 train/test split using the training dataset (with the 20% being used to calibrate)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Prepare validation set where validation is being performed\n",
    "if not trainingOnly:\n",
    "    X_val, Y_val = load_data_splits(validationDataset, features)\n",
    "    X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)\n",
    "    X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "elif trainingOnly:\n",
    "    #If not an evaluation site, use test set as the validation set\n",
    "    X_val = X_test\n",
    "    Y_val = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "90ed8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "#Define Deep Learning Model\n",
    "\n",
    "modelType = \"DL\"\n",
    "seeds=[10,20,100,223]\n",
    "tf.random.set_seed(seeds[1])\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self,nodes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d1=layers.Dense(nodes,activation='relu')\n",
    "        self.dropout1=tf.keras.layers.Dropout(0.5)\n",
    "        self.d5=layers.Dense(1,activation='sigmoid')\n",
    "\n",
    "    def forward(self, x,train=False):\n",
    "        x = self.d1(x)\n",
    "        x = self.dropout1(x,training=train)\n",
    "        x = self.d5(x)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.forward(inputs,train=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "model=Model(10)\n",
    "model.build((None,27))\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(),metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "23129330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.6142 - auc: 0.6715 - val_loss: 0.4120 - val_auc: 0.7215\n",
      "Epoch 2/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.3340 - auc: 0.7669 - val_loss: 0.2991 - val_auc: 0.7778\n",
      "Epoch 3/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2622 - auc: 0.8076 - val_loss: 0.2624 - val_auc: 0.8034\n",
      "Epoch 4/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2356 - auc: 0.8278 - val_loss: 0.2480 - val_auc: 0.8201\n",
      "Epoch 5/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2231 - auc: 0.8430 - val_loss: 0.2394 - val_auc: 0.8311\n",
      "Epoch 6/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2152 - auc: 0.8532 - val_loss: 0.2335 - val_auc: 0.8388\n",
      "Epoch 7/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2093 - auc: 0.8597 - val_loss: 0.2296 - val_auc: 0.8452\n",
      "Epoch 8/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2049 - auc: 0.8676 - val_loss: 0.2244 - val_auc: 0.8534\n",
      "Epoch 9/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2008 - auc: 0.8725 - val_loss: 0.2213 - val_auc: 0.8575\n",
      "Epoch 10/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1975 - auc: 0.8770 - val_loss: 0.2189 - val_auc: 0.8609\n",
      "Epoch 11/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1946 - auc: 0.8800 - val_loss: 0.2166 - val_auc: 0.8633\n",
      "Epoch 12/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1920 - auc: 0.8845 - val_loss: 0.2150 - val_auc: 0.8662\n",
      "Epoch 13/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1895 - auc: 0.8865 - val_loss: 0.2131 - val_auc: 0.8697\n",
      "Epoch 14/20\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 0.1874 - auc: 0.8898 - val_loss: 0.2123 - val_auc: 0.8712\n",
      "Epoch 15/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1854 - auc: 0.8917 - val_loss: 0.2106 - val_auc: 0.8729\n",
      "Epoch 16/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1838 - auc: 0.8935 - val_loss: 0.2097 - val_auc: 0.8729\n",
      "Epoch 17/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1822 - auc: 0.8950 - val_loss: 0.2086 - val_auc: 0.8737\n",
      "Epoch 18/20\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.1808 - auc: 0.8973 - val_loss: 0.2087 - val_auc: 0.8730\n",
      "Epoch 19/20\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.1793 - auc: 0.8989 - val_loss: 0.2076 - val_auc: 0.8729\n",
      "Epoch 20/20\n",
      "196/196 [==============================] - 2s 8ms/step - loss: 0.1783 - auc: 0.8997 - val_loss: 0.2070 - val_auc: 0.8751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x299a0a470>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='./local_models/OUH.h5',save_weights_only=True,monitor='val_auc',mode='max',save_best_only=True)\n",
    "#model.fit(tf.cast(X_train.values, tf.float32), tf.cast(Y_train.values, tf.float32), epochs=20, batch_size=32,verbose=1)\n",
    "\n",
    "model.fit(X_train.values, Y_train.values, validation_data=(X_test.values,Y_test.values), epochs=20, batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4908c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model.predict_on_batch(X_val).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e2b89f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01967058],\n",
       "       [0.02115424],\n",
       "       [0.03196299],\n",
       "       ...,\n",
       "       [0.04715247],\n",
       "       [0.42735675],\n",
       "       [0.00619457]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "68c7d7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01967058],\n",
       "       [0.02115424],\n",
       "       [0.03196299],\n",
       "       ...,\n",
       "       [0.04715247],\n",
       "       [0.42735675],\n",
       "       [0.00619457]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "15340b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(warm_start=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(warm_start=True)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    max_iter=100,  # local epoch\n",
    "    warm_start=True,  # prevent refreshing weights when fitting\n",
    ")\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f0c3b2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01089234, 0.02301274, 0.0094173 , ..., 0.03234528, 0.18560006,\n",
       "       0.01425247])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0f24ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def find_threshold_at_metric(model,inputs,outputs,best_threshold,metric_of_interest,value_of_interest,results_df,fold_number,error, match_number, modelType = \"LR\"):\n",
    "        ground_truth = outputs['eval']\n",
    "\n",
    "        \"\"\" Probability Values for Predictions \"\"\"\n",
    "        if modelType == \"LR\":\n",
    "            probs = model.predict_proba(inputs['eval'])[:,1]\n",
    "        elif modelType == \"DL\":\n",
    "            probs = model.predict_on_batch(inputs['eval'])\n",
    "            \n",
    "        threshold_metrics = pd.DataFrame(np.zeros((500,8)),index=np.linspace(0,1,500),columns=['Recall','Precision','F1-Score','Accuracy','Specificity','PPV','NPV','AUC'])\n",
    "        prev = 1/(match_number+1)\n",
    "        for t in np.linspace(0,1,500):\n",
    "            preds = np.where(probs>t,1,0)\n",
    "            recall = recall_score(ground_truth,preds,zero_division=0)\n",
    "            accuracy = accuracy_score(ground_truth,preds)\n",
    "            auc = roc_auc_score(ground_truth,probs)\n",
    "            tn, fp, fn, tp = confusion_matrix(ground_truth,preds).ravel()\n",
    "            specificity = tn/(tn+fp)\n",
    "            ppv = (recall* (prev))/(recall * prev + (1-specificity) * (1-prev))\n",
    "            precision = ppv\n",
    "            f1score = 2*(precision*recall)/(precision+recall)\n",
    "            if tn== 0 and fn==0:\n",
    "                npv = 0\n",
    "            else:\n",
    "                npv = (specificity* (1-prev))/(specificity * (1-prev) + (1-recall) * (prev))\n",
    "            threshold_metrics.loc[t,['Recall','Precision','F1-Score','Accuracy','Specificity','PPV','NPV','AUC']] = [recall,precision,f1score,accuracy,specificity,ppv,npv,auc]\n",
    "\n",
    "        \"\"\" Identify Results that Satisfy Constraints and Best Threshold \"\"\"\n",
    "     #   value_of_interest = threshold_metrics.loc[:,metric_of_interest].max()\n",
    "        condition1 = threshold_metrics.loc[:,metric_of_interest] < value_of_interest + error\n",
    "        condition2 = threshold_metrics.loc[:,metric_of_interest] > value_of_interest - error\n",
    "        combined_condition = condition1 & condition2\n",
    "        if metric_of_interest == 'Recall':\n",
    "            sort_col = 'Precision'\n",
    "        elif metric_of_interest == 'Precision':\n",
    "            sort_col = 'Recall'\n",
    "        elif metric_of_interest == 'F1-Score':\n",
    "            sort_col = 'F1-Score'\n",
    "        sorted_results = threshold_metrics[combined_condition].sort_values(by=sort_col,ascending=False)\n",
    "      #  print(sorted_results)\n",
    "        if len(sorted_results) > 0:\n",
    "            \"\"\" Only Record Value if Condition is Satisfied \"\"\"\n",
    "            results_df.loc[['Recall','Precision','F1-Score','Accuracy','Specificity','PPV','NPV','AUC'],fold_number] = sorted_results.iloc[0,:]\n",
    "            best_threshold.iloc[fold_number] = sorted_results.iloc[0,:].name\n",
    "        else:\n",
    "            print('No Threshold Found for Constraint!')\n",
    "\n",
    "        return best_threshold, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "38e087df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(np.zeros((len(metric_names),1)),index=metric_names)\n",
    "best_threshold = pd.Series(np.zeros((1)))\n",
    "inputs,outputs = dict(), dict()\n",
    "inputs['eval'], outputs['eval'] = [X_test,Y_test]\n",
    "thresholds_on_test, results_df = find_threshold_at_metric(model, inputs, outputs, best_threshold, \"Recall\", targetSensitivity, results_df, 0, 0.04, match_number,\"DL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2b543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
